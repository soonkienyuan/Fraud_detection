{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "The project requires all the followings:\n",
    "1. **Executive Summary**\n",
    "   \n",
    "    Description of the selected project, problem to be solved, and basic description of the data set from the sources.\n",
    "\n",
    "2. **Summary of the Project Context and Objectives**\n",
    "   \n",
    "    Summarise your project context and list the objectives covered in your project.\n",
    "\n",
    "3. **Methodology**\n",
    "\n",
    "    Create a data pipeline to depict your sequence of actions that move data from a source to a destination. Show and explain all phases or steps involved in the data mining process, including the data preprocessing (ETL/ ELT), modelling, evaluation and deployment in your project development.\n",
    "\n",
    "4. **Results and Discussion**\n",
    "\n",
    "    Produce and explain all codes, GUI, graphs, diagrams, or any visualisation output from the project development.\n",
    "\n",
    "5. **Conclusion**\n",
    "\n",
    "    Conclude your project and discuss some contributions to society, the environment, systems, etc.\n",
    "\n",
    "6. **References**\n",
    "\n",
    "    Cite every single reference used in completing the project. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "## Problem to be solved\n",
    "In this competition's datasets, you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud. \n",
    "For each TransactionID in the test set, you must predict a probability for the isFraud variable.\n",
    "\n",
    "## Dataset Basic Description\n",
    "Please note that in this report, we use the zip that include all dataset csv together. The data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information. As described by host,\n",
    "\n",
    "Transaction Table *\n",
    "- TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n",
    "- TransactionAMT: transaction payment amount in USD\n",
    "- ProductCD: product code, the product for each transaction\n",
    "- card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n",
    "- addr: address\n",
    "- dist: distance\n",
    "- P_ and (R__) emaildomain: purchaser and recipient email domain\n",
    "- C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
    "- D1-D15: timedelta, such as days between previous transaction, etc.\n",
    "- M1-M9: match, such as names on card and address, etc.\n",
    "- Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
    "\n",
    "Categorical Features:\n",
    "- ProductCD\n",
    "- card1 - card6\n",
    "- addr1, addr2\n",
    "- P_emaildomain\n",
    "- R_emaildomain\n",
    "- M1 - M9\n",
    "\n",
    "Identity Table *\n",
    "\n",
    "Variables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions.\n",
    "They're collected by Vesta’s fraud protection system and digital security partners.\n",
    "(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n",
    "\n",
    "Categorical Features:\n",
    "- DeviceType\n",
    "- DeviceInfo\n",
    "- id_12 - id_38\n",
    "\n",
    "## Additional Information about datasets\n",
    "1. [Further Information and related discussion](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203)\n",
    "2. [Kaggler's insight](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#610146)\n",
    "3. [Labelling logic](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#589276)\n",
    "    > Is this all real data, and if so, how confident are you that all fraud instances are correctly labeled? I ask because I see some suspicious rows that look like undetected fraud, or synthetic rows added to confuse one of the more obvious fraud detection algorithms.\n",
    "    >\n",
    "    >But then again, with such a large dataset one can expect to find some very improbable coincidences within it.\n",
    "\n",
    "\n",
    "   > It's a good question.\n",
    "   >\n",
    "   > Yes, they're all real data, no synthetic data. The logic of our labeling is define reported chargeback on the card as fraud transaction (isFraud=1) and transactions posterior to it with either user account, email address or billing address directly linked to these attributes as fraud too. If none of above is reported and found beyond 120 days, then we define as legit transaction (isFraud=0).\n",
    "    However, in real world fraudulent activity might not be reported, e.g. cardholder was unaware, or forgot to report in time and beyond the claim period, etc. In such cases, supposed fraud might be labeled as legit, but we never could know of them. Thus, we think they're unusual cases and negligible portion.\n",
    "4. [Main Ideas from Grandmaster's EDA](https://www.kaggle.com/code/cdeotte/eda-for-columns-v-and-id/notebook)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of EDA\n",
    "\n",
    "The main purpose of exploratory data analysis (EDA) determine strategies of the imputation, encoding, aggregration, reduction and transformation for each feature.\n",
    "\n",
    "There are too much data to be fed into to later model development. Hence, feature selection is essential here. Feature selection techniques adopted are missing value analysis and correlation analysis.\n",
    "\n",
    "Firstly, we group the features based on whether their missing values are happening together. This results multiple pages for example v1 until v11 are related via their missing values.\n",
    "\n",
    "Since it is computationally infeasible to compute the correlation matrix of all 300++ vesta rich features, we compute correlation matrix of smaller pages that are found earlier. From that, we futher group the features based correlation value (i.e.: see `naive_reduce` function). Counting and temporal attributes are also grouped via this correlation selection approach. The grouped variables will be aggregrated into a sinlge variable in preprocessing step.\n",
    "\n",
    "For identity dataset, we see that that identity info and fraudelent transaction are not strongly correlated (neither positve nor negative). Furthermore, not all identity info are provided, then we decide not to include them as input features for model developed.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "\n",
    "\n",
    "The rationales behind the choices of preprocessing algorithms are made based on EDA. We adopt ELT steps that is extract, load and transform. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELT Pipeline of Preprocessing \n",
    "We only apply few transformation at the data reading (e.g.: switching index, categorical typing and data enrichment), then the data will be fed into the preprocessing pipeline we have developed using sklearn library. The preprocessing pipeline only involves label encoding, missing value imputation, reduction via aggregration and numerical normalization.\n",
    "\n",
    "> The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the transformers list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the passthrough keyword. Those columns specified with passthrough are added at the right to the output of the transformers.\n",
    "\n",
    "In developing pipeline under `sklearn` framework, `ColumnTransformer` apply transformations parallelly whereas `Pipeline` apply transformations sequentially. Thus, we mix them to achive desired overall transformation. \n",
    "\n",
    "The results are a 2D array for features and a 1D array input target. The order of resultant feature array does not order as original data frame as mentioned by `sklearn` documentation. \n",
    "\n",
    "Using the results from EDA section, we will reduce features based on basis of correlation (i.e.: subsets group). To impute missing value and aggregate grouped variables, we have defined a basic unit constructor `impute_reduce_pipe`. `correlated_group_reducer` is built upon the `impute_reduce_pipe`.\n",
    "\n",
    "Initially, we want the reducer adjust its imputation and aggregration strategies based on the grouped variables. For example, for one variable, we can skip; for 2 to 5, we can take the average; for more than 5, we do dimension reduction PCA. However, due to the size of dataset, KNN imputation and dimension reduction techniques don't scale well; too slow. Hence, we don't adopt such strategy.\n",
    "\n",
    "As a result, we reduce 300++ features to 178 input features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rationales of developing pipeline\n",
    "Suppose that we've 10 input feature and 1 categorical target. The input feature is a random mix of categorical and continuous feature. We develop a code that impute and encode each feature accordingly and one-hot encode the categorical feature. We can use `ColumnTransformer` and `Pipeline` to describe such processing step so that the preprocessor receives and transform any data frame that has the same format. Furthermore, the preprocessor 'should' known how to inverse transform each encoded feature. Unfortunately, the complete inversible pipeline implementation is cumbersome and time consuming if using current `sklearn` framework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling and Evaluation\n",
    "> Champion's solution - https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/111308\n",
    "\n",
    "Refering to the competition champion's solution, XGB model perform very well. Therefore, we adopt the same model. Once we done model development and training, we submit the `submission.csv` to Kaggle viewing the percision score. \n",
    "\n",
    "Since the target feature distribution is heavily imbalanced, precision score is used rather than accuracy.\n",
    "\n",
    "Trying different model with different parameters (e.g.: random forest and decision tree), the precision score is still capped at 0.70 whereas top 100 solutions all have above 0.95 score. The low percision score is due to we don't include identity dataset in model training. Also, we don't aggressively engineer our features via time consistency checking.\n",
    "\n",
    "## Deployment\n",
    "\n",
    "The `deployment.py` plot a windowed time series chart whose peak indicate a suspect transaction. It involves using `ELT.py`'s data generator to simulate the process of fetching data from other sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2324a66782fee6ab9f0bdb3c9e79ee636ed86487484f245c4444858429ce7730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
